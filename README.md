# Heading 1
## Heading 2
### Heading 3

**Bold Text**

*Italic Text*

- List item 1
- List item 2
- List item 3

[Link to GitHub](https://github.com/)

![Image Alt Text](https://via.placeholder.com/150)

# Code block
print("Hello, World!")

# Introduction
This work aims to demonstrate a process for quantifying a Granger Causality relationship between nodes in an ictal network, to contribute to a larger system used to find the central nodes that cause seizures.

Given two time series (in this case, two channels of sEEG data), a Granger Causality test will quantify a causal relationship between them. **I.E. if a change in one, is causing a change in the other.**


# Process

## Simulating Data
Creating some sample/simulated data. The complete dataset contains 7 seconds at 1024hz. 
```python
r.append(0.5 * r[-1] + np.random.rand())  # Append a new point: 50% of the last point plus a random value, simulating realistic, noisy data progression.
```
Forcing causality in the simulated data. First, a given channel is copied to another row, and some random noise is added. 
```python
independent_channel = 1
dependent_channel = 5
#  copy each datapoint and add noise
for sample in range(np.shape(data)[1]):
        forced_data[dependent_channel, sample] = data[independent_channel, sample] + (noise * (np.random.rand() - 0.5))
```
Next, the data is shifted by a certain amount of lags (datapoints)
```python
lag = 2
#  roll dataset by given lags
forced_data[dependent_channel, :] = np.roll(data[dependent_channel, :], lag)
```
The result is a straightforward causal relationship, where a change in the first channel(1) causes a change in the second channel (5), after a set number of lags(2). 

![Simulated vs Real Data](https://github.com/sbockfind/main/blob/main/Figure%202024-07-10%20192428.png)

Another method of forcing causality. Some of the information of the dependent channel was kept by averaging the dependent and independent channels, again adding noise and 'rolling' the data.

```python
independent_channel = 1
dependent_channel = 6
lag = 2
#  average the data points and add noise
for sample in range(np.shape(data)[1]):
  forced_data[dependent_channel, sample] = ((0.5 * data[dependent_channel, sample]) + 
                                            (0.5 * data[independent_channel, sample]) +
                                            (noise * (np.random.rand() - 0.5)))
#  roll data by given lags
forced_data[dependent_channel, :] = np.roll(data[dependent_channel, :], lag)
```




The data is first cropped down to a few channels from each electrode (e.g. LFIa01-LFIa02, LFIa08-LFIa09, and LFIa15-LFIa16) and 4 seconds of data: 2 seconds on either side of the seizure onset. The cropped data contains the resected area corresponding to: RFM08-RFM09, RFM09-RFM10. Each combination of channels is run through a Likelihood Ratio (LR) test, pulled out of the statsmodels grangercausalitytests() function. Only the highest likelihood and the order (or lag) at which it occurs are kept.
	When the highest likelihood is significant at 95% confidence, a GC test is run for that combination at the order recorded. (the function used is calc_epoch_granger_causality_ij(), which uses the nitime GrangerAnalyzer())
	The program then compares centrality measures event_critical_nodes() and nxâ€™s weighted_out_degree_centrality.
